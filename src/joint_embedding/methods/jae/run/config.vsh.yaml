functionality:
  name: jae
  namespace: joint_embedding_methods

  # metadata for your method
  description: In brief, we built an autoencoder for joint embedding (JAE). Each modality will first be SVD transformed and concatenated together. The major difference from standard AE is that we incorporated the information from cell annotations (e.g., cell label, cell cycle score, and cell batch) to constrain the structure of latent features. We desire that some latent features predict the cell type information, some features predict the cell cycle score. Noticeably, for feature corresponding to batch effect, we want it to predict the batch label as randomly as possible to potentially eliminate the batch effect. There are also several nodes that have no constrain at all to ensure the flexibility of neural network.
  info:
    method_label: "JAE"
    submission_id: "170936/171079"
    team_name: Amateur
    project_url: https://github.com/kimmo1019/JAE

  authors:
    - name: Qiao Liu 
      email: liuqiao@stanford.edu
      roles: [ author, maintainer ]
      props: { github: kimmo1019, orcid: "0000-0002-9781-3360", url: "http://liuqiao.me" }
    - name: Wanwen Zeng
      email: wanwen@stanford.edu
      roles: [ author ]
      props: { github: wanwenzeng, orcid: "0000-0003-3426-0890", url: "https://scholar.google.com/citations?user=MbeOhkgAAAAJ&hl=zh-CN" }
    - name: Chencheng Xu
      roles: [ author ]
      props: { github: Zoesgithub, orcid: "0000-0002-2262-6966" }

  # parameters
  arguments:
    # required inputs
    - name: "--input_mod1"
      type: "file"
      example: "dataset_mod1.h5ad"
      description: Modality 1 dataset.
      required: true
    - name: "--input_mod2"
      type: "file"
      example: "dataset_mod2.h5ad"
      description: Modality 2 dataset.
      required: true
    - name: "--input_pretrain"
      type: "file"
      example: "pretrain_model"
      description: Path to the directory containing a pretrained model.
      required: true
    # required outputs
    - name: "--output"
      type: "file"
      direction: "output"
      example: "output.h5ad"
      description: Data for all cells in mod1 and mod2 embedded to â‰¤100 dimensions.
      required: true

  # files your script needs
  resources:
    - type: python_script
      path: script.py
    - path: '../resources/utils.py'

# target platforms
platforms:
  - type: docker
    image: tensorflow/tensorflow:latest-gpu
    run_args: [ "--gpus all" ]
    setup:
      - type: python
        packages:
          - anndata
          - umap-learn
          - scanpy
  - type: nextflow
    labels: [ vhighmem, vhightime, vhighcpu, gpu ]
